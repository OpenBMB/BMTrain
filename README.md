<div align="center">

<h1>ğŸš„ BMTrain</h1>

------

<p align="center">

<a href='https://bmtrain.readthedocs.io/en/latest/?badge=latest'>
    <img src='https://readthedocs.org/projects/bmtrain/badge/?version=latest' alt='Documentation Status' />
</a>

<a href="https://github.com/OpenBMB/BMTrain/releases">
    <img alt="GitHub release (latest by date including pre-releases)" src="https://img.shields.io/github/v/release/OpenBMB/BMTrain?include_prereleases">
</a>

<a href="https://github.com/OpenBMB/BMTrain/blob/main/LICENSE">
    <img alt="GitHub" src="https://img.shields.io/github/license/OpenBMB/BMTrain">
</a>

</p>

</div>


## 1. å®‰è£…

#### From PyPI (recommended)

```shell
$ pip install bmtrain
```

#### From source

```
$ git clone https://github.com/OpenBMB/BMTrain.git
$ cd BMTrain
$ python setup.py install
```

## 2. ä½¿ç”¨

### Step 1: å¯ç”¨ bmtrain

è¦ä½¿ç”¨bmtrainéœ€è¦åœ¨ä»£ç ä¸­å¼•å…¥`bmtrain`å·¥å…·åŒ…ï¼Œå¹¶åœ¨ä»£ç çš„å¼€å¤´ä½¿ç”¨`bmtrain.init_distributed`

```python
import bmtrain as bmt
bmt.init_distributed(
    seed=0,
    # ...
)
```

**æ³¨æ„ï¼š** ä½¿ç”¨`bmtrain`æ—¶è¯·ä¸è¦ä½¿ç”¨`pytorch`è‡ªå¸¦çš„`distributed`æ¨¡å—ï¼ŒåŒ…æ‹¬`torch.distributed.init_process_group`ä»¥åŠç›¸å…³é€šä¿¡å‡½æ•°ã€‚

### Step 2: ä½¿ç”¨ ZeRO3 ä¼˜åŒ–

ä½¿ç”¨ZeRO3ä¼˜åŒ–éœ€è¦å¯¹æ¨¡å‹ä»£ç è¿›è¡Œç®€å•æ›¿æ¢ï¼š

* `torch.nn.Module` -> `bmtrain.DistributedModule`
* `torch.nn.Parameter` -> `bmtrain.DistributedParameter`

å¹¶åœ¨åˆé€‚çš„æ¨¡å—ä¸Šä½¿ç”¨`Checkpointing`ã€‚

**åŸå§‹ä»£ç ï¼š**

```python
import torch
class MyModule(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.param = torch.nn.Parameter(torch.empty(1024))
        self.module_list = torch.nn.ModuleList([
            SomeTransformerBlock(),
            SomeTransformerBlock(),
            SomeTransformerBlock()
        ])
    
    def forward(self):
        x = self.param
        for module in self.module_list:
            x = module(x, 1, 2, 3)
        return x

```

**æ›¿æ¢åä»£ç ï¼š**

```python
import torch
import bmtrain as bmt
class MyModule(bmt.DistributedModule):
    def __init__(self):
        super().__init__()
        self.param = bmt.DistributedParameter(torch.empty(1024))
        self.module_list = torch.nn.ModuleList([
            bmt.CheckpointBlock(SomeTransformerBlock()),
            bmt.CheckpointBlock(SomeTransformerBlock()),
            bmt.CheckpointBlock(SomeTransformerBlock())
        ])
    
    def forward(self):
        x = self.param
        for module in self.module_list:
            x = module(x, 1, 2, 3)
        return x
    
```

### Step 3: é€šä¿¡ä¼˜åŒ–

ä¸ºäº†è¿›ä¸€æ­¥ç¼©çŸ­é€šä¿¡é¢å¤–å¼€é”€ï¼Œå°†é€šä¿¡ä¸è¿ç®—æ—¶é—´é‡å ï¼Œå¯ä»¥ä½¿ç”¨`TransformerBlockList`æ¥è¿›ä¸€æ­¥ä¼˜åŒ–ã€‚
åœ¨ä½¿ç”¨æ—¶éœ€è¦å¯¹ä»£ç è¿›è¡Œç®€å•æ›¿æ¢ï¼š

* `torch.nn.ModuleList` -> `bmtrain.TransformerBlockList`
* `for module in self.module_list: x = module(x, ...)` -> `x = self.module_list(x, ...)`

**åŸå§‹ä»£ç ï¼š**

```python
import torch
import bmtrain as bmt
class MyModule(bmt.DistributedModule):
    def __init__(self):
        super().__init__()
        self.param = bmt.DistributedParameter(torch.empty(1024))
        self.module_list = torch.nn.ModuleList([
            bmt.CheckpointBlock(SomeTransformerBlock()),
            bmt.CheckpointBlock(SomeTransformerBlock()),
            bmt.CheckpointBlock(SomeTransformerBlock())
        ])
    
    def forward(self):
        x = self.param
        for module in self.module_list:
            x = module(x, 1, 2, 3)
        return x
    
```

**æ›¿æ¢åä»£ç ï¼š**

```python
import torch
import bmtrain as bmt
class MyModule(bmt.DistributedModule):
    def __init__(self):
        super().__init__()
        self.param = bmt.DistributedParameter(torch.empty(1024))
        self.module_list = bmt.TransformerBlockList([
            bmt.CheckpointBlock(SomeTransformerBlock()),
            bmt.CheckpointBlock(SomeTransformerBlock()),
            bmt.CheckpointBlock(SomeTransformerBlock())
        ])
    
    def forward(self):
        x = self.param
        x = self.module_list(x, 1, 2, 3)
        return x
    
```

### Step 4: è¿è¡Œåˆ†å¸ƒå¼è®­ç»ƒä»£ç 

bmtrainæ”¯æŒpytorchåŸç”Ÿçš„åˆ†å¸ƒå¼è®­ç»ƒå¯åŠ¨å™¨ï¼š

#### torch.distributed.launch
```shell
$ python3 -m torch.distributed.launch --master_addr ${MASTER_ADDR} --master_port ${MASTER_PORT} --nproc_per_node ${GPU_PER_NODE} --nnodes ${NNODES} --node_rank ${NODE_RANK} train.py
```

#### torchrun

```shell
$ torchrun --nnodes=${NNODES} --nproc_per_node=${GPU_PER_NODE} --rdzv_id=1 --rdzv_backend=c10d --rdzv_endpoint=${MASTER_ADDR}:${MASTER_PORT} train.py
```

æ›´å¤šä¿¡æ¯è¯·å‚è€ƒpytorchå®˜æ–¹æ–‡æ¡£ï¼š![Launch utility](https://pytorch.org/docs/stable/distributed.html#launch-utility)

## 3. å…¶å®ƒè¯´æ˜

`BMTrain`å·¥å…·åŒ…å¯¹pytorchè¿›è¡Œäº†åº•å±‚ä¿®æ”¹ï¼Œå¦‚æœä½ çš„ç¨‹åºè¾“å‡ºäº†æ„æ–™ä¹‹å¤–çš„ç»“æœï¼Œå¯ä»¥åœ¨issueä¸­æäº¤ç›¸å…³ä¿¡æ¯ã€‚

æ›´å¤šä¾‹å­è¯·å‚è€ƒ *examples* æ–‡ä»¶å¤¹ã€‚

