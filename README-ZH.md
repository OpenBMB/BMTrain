<div align="center">

<h1>ğŸš„ BMTrain</h1>

**å¤§æ¨¡å‹é«˜æ•ˆè®­ç»ƒå·¥å…·åŒ…**

<p align="center">
  <a href="#æ€»è§ˆ">æ€»è§ˆ</a> â€¢ <a href="#æ–‡æ¡£">æ–‡æ¡£</a> â€¢ <a href="#å®‰è£…">å®‰è£…</a> â€¢ <a href="#ä½¿ç”¨è¯´æ˜">ä½¿ç”¨è¯´æ˜</a> â€¢ <a href="#æ€§èƒ½">æ€§èƒ½</a> â€¢ <a href="./README.md" target="_blank">English</a>
<br>
</p>

<p align="center">

<a href='https://bmtrain.readthedocs.io/en/latest/?badge=latest'>
    <img src='https://readthedocs.org/projects/bmtrain/badge/?version=latest' alt='Documentation Status' />
</a>

<a href="https://github.com/OpenBMB/BMTrain/releases">
    <img alt="GitHub release (latest by date including pre-releases)" src="https://img.shields.io/github/v/release/OpenBMB/BMTrain?include_prereleases">
</a>

<a href="https://github.com/OpenBMB/BMTrain/blob/main/LICENSE">
    <img alt="GitHub" src="https://img.shields.io/github/license/OpenBMB/BMTrain">
</a>

</p>

</div>

## æœ€æ–°åŠ¨æ€
- 2022/03/16 [0.1.1](https://github.com/OpenBMB/BMTrain/releases/tag/0.1.1) BMTrain å…¬å¼€å‘å¸ƒäº†ç¬¬ä¸€ä¸ªç¨³å®šç‰ˆæœ¬ï¼Œä¿®å¤äº†betaç‰ˆæœ¬ä¸­çš„ä¸€äº›é—®é¢˜.
- 2022/02/11 [0.0.15](https://github.com/OpenBMB/BMTrain/releases/tag/0.0.15) BMTrain å…¬å¼€å‘å¸ƒäº†ç¬¬ä¸€ä¸ª beta ç‰ˆæœ¬ã€‚

<div id="æ€»è§ˆ"></div>

## æ€»è§ˆ

BMTrain æ˜¯ä¸€ä¸ªé«˜æ•ˆçš„å¤§æ¨¡å‹è®­ç»ƒå·¥å…·åŒ…ï¼Œå¯ä»¥ç”¨äºè®­ç»ƒæ•°ç™¾äº¿å‚æ•°çš„å¤§æ¨¡å‹ã€‚BMTrain å¯ä»¥åœ¨åˆ†å¸ƒå¼è®­ç»ƒæ¨¡å‹çš„åŒæ—¶ï¼Œèƒ½å¤Ÿä¿æŒä»£ç çš„ç®€æ´æ€§ã€‚

<div id="æ–‡æ¡£"></div>

## æ–‡æ¡£
æˆ‘ä»¬çš„[æ–‡æ¡£](https://bmtrain.readthedocs.io/en/latest/index.html) æä¾›äº†å…³äºå·¥å…·åŒ…çš„æ›´å¤šä¿¡æ¯ã€‚

<div id="å®‰è£…"></div>

## å®‰è£…

- ç”¨ pip å®‰è£…ï¼ˆæ¨èï¼‰: ``pip install bmtrain``

- ä»æºä»£ç å®‰è£…: ä¸‹è½½å·¥å…·åŒ…ï¼Œç„¶åè¿è¡Œ ``python setup.py install``

å®‰è£… BMTrain å¯èƒ½éœ€è¦èŠ±è´¹æ•°åˆ†é’Ÿçš„æ—¶é—´ï¼Œå› ä¸ºåœ¨å®‰è£…æ—¶éœ€è¦ç¼–è¯‘ c/cuda æºä»£ç ã€‚
æˆ‘ä»¬æ¨èç›´æ¥åœ¨è®­ç»ƒç¯å¢ƒä¸­ç¼–è¯‘ BMTrainï¼Œä»¥é¿å…ä¸åŒç¯å¢ƒå¸¦æ¥çš„æ½œåœ¨é—®é¢˜ã€‚


<div id="ä½¿ç”¨è¯´æ˜"></div>

## è¯´æ˜

### æ­¥éª¤ 1: å¯ç”¨ BMTrain

é¦–å…ˆï¼Œä½ éœ€è¦åœ¨ä»£ç å¼€å¤´åˆå§‹åŒ– BMTrainã€‚æ­£å¦‚åœ¨ä½¿ç”¨ PyTorch çš„åˆ†å¸ƒå¼è®­ç»ƒæ¨¡å—éœ€è¦åœ¨ä»£ç å¼€å¤´ä½¿ç”¨ **init_process_group**ä¸€æ ·ï¼Œä½¿ç”¨ BMTrain éœ€è¦åœ¨ä»£ç å¼€å¤´ä½¿ç”¨ **init_distributed**ã€‚

```python
import bmtrain as bmt
bmt.init_distributed(
    seed=0,
    # ...
)
```

**æ³¨æ„ï¼š** ä½¿ç”¨ BMTrain æ—¶è¯·ä¸è¦ä½¿ç”¨ PyTorch è‡ªå¸¦çš„ `distributed` æ¨¡å—ï¼ŒåŒ…æ‹¬ `torch.distributed.init_process_group` ä»¥åŠç›¸å…³é€šä¿¡å‡½æ•°ã€‚

### æ­¥éª¤ 2: ä½¿ç”¨ ZeRO-3 ä¼˜åŒ–

ä½¿ç”¨ZeRO3ä¼˜åŒ–éœ€è¦å¯¹æ¨¡å‹ä»£ç è¿›è¡Œç®€å•æ›¿æ¢ï¼š

* `torch.nn.Module` -> `bmtrain.DistributedModule`
* `torch.nn.Parameter` -> `bmtrain.DistributedParameter`

And wrap the transformer blocks with `bmtrain.CheckpointBlock`.
å¹¶åœ¨ transformer æ¨¡å—ä¸Šä½¿ç”¨ `bmtrain.CheckpointBlock`ã€‚

ä¸‹é¢æ˜¯ä¸€ä¸ªä¾‹å­ï¼š

**åŸå§‹ä»£ç **

```python
import torch
class MyModule(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.param = torch.nn.Parameter(torch.empty(1024))
        self.module_list = torch.nn.ModuleList([
            SomeTransformerBlock(),
            SomeTransformerBlock(),
            SomeTransformerBlock()
        ])
    
    def forward(self):
        x = self.param
        for module in self.module_list:
            x = module(x, 1, 2, 3)
        return x

```

**æ›¿æ¢åä»£ç **

```python
import torch
import bmtrain as bmt
class MyModule(bmt.DistributedModule): # ä¿®æ”¹è¿™é‡Œ
    def __init__(self):
        super().__init__()
        self.param = bmt.DistributedParameter(torch.empty(1024)) # ä¿®æ”¹è¿™é‡Œ
        self.module_list = torch.nn.ModuleList([
            bmt.CheckpointBlock(SomeTransformerBlock()), # ä¿®æ”¹è¿™é‡Œ
            bmt.CheckpointBlock(SomeTransformerBlock()), # ä¿®æ”¹è¿™é‡Œ
            bmt.CheckpointBlock(SomeTransformerBlock())  # ä¿®æ”¹è¿™é‡Œ
        ])
    
    def forward(self):
        x = self.param
        for module in self.module_list:
            x = module(x, 1, 2, 3)
        return x
    
```

### æ­¥éª¤ 3: é€šä¿¡ä¼˜åŒ–

ä¸ºäº†è¿›ä¸€æ­¥ç¼©çŸ­é€šä¿¡é¢å¤–å¼€é”€ï¼Œå°†é€šä¿¡ä¸è¿ç®—æ—¶é—´é‡å ï¼Œå¯ä»¥ä½¿ç”¨`TransformerBlockList`æ¥è¿›ä¸€æ­¥ä¼˜åŒ–ã€‚

åœ¨ä½¿ç”¨æ—¶éœ€è¦å¯¹ä»£ç è¿›è¡Œç®€å•æ›¿æ¢ï¼š

* `torch.nn.ModuleList` -> `bmtrain.TransformerBlockList`
* `for module in self.module_list: x = module(x, ...)` -> `x = self.module_list(x, ...)`

**åŸå§‹ä»£ç **

```python
import torch
import bmtrain as bmt
class MyModule(bmt.DistributedModule):
    def __init__(self):
        super().__init__()
        self.param = bmt.DistributedParameter(torch.empty(1024))
        self.module_list = torch.nn.ModuleList([
            bmt.CheckpointBlock(SomeTransformerBlock()),
            bmt.CheckpointBlock(SomeTransformerBlock()),
            bmt.CheckpointBlock(SomeTransformerBlock())
        ])
    
    def forward(self):
        x = self.param
        for module in self.module_list:
            x = module(x, 1, 2, 3)
        return x
    
```

**æ›¿æ¢åä»£ç **

```python
import torch
import bmtrain as bmt
class MyModule(bmt.DistributedModule):
    def __init__(self):
        super().__init__()
        self.param = bmt.DistributedParameter(torch.empty(1024))
        self.module_list = bmt.TransformerBlockList([ # ä¿®æ”¹è¿™é‡Œ
            bmt.CheckpointBlock(SomeTransformerBlock()),
            bmt.CheckpointBlock(SomeTransformerBlock()),
            bmt.CheckpointBlock(SomeTransformerBlock())
        ])
    
    def forward(self):
        x = self.param
        x = self.module_list(x, 1, 2, 3) # ä¿®æ”¹è¿™é‡Œ
        return x
    
```

### æ­¥éª¤ 4: è¿è¡Œåˆ†å¸ƒå¼è®­ç»ƒä»£ç 

BMTrain ä½¿ç”¨ PyTorch åŸç”Ÿçš„åˆ†å¸ƒå¼è®­ç»ƒå¯åŠ¨å™¨ï¼Œä½ å¯ä»¥æ ¹æ® PyTorch ç‰ˆæœ¬é€‰æ‹©ä¸‹åˆ—å‘½ä»¤ä¸­çš„ä¸€ä¸ªã€‚

* `${MASTER_ADDR}` ä¸ºä¸»èŠ‚ç‚¹çš„ IP åœ°å€
* `${MASTER_PORT}` ä¸ºä¸»èŠ‚ç‚¹çš„ç«¯å£
* `${NNODES}` ä¸ºèŠ‚ç‚¹æ•°é‡
* `${GPU_PER_NODE}` ä¸ºæ¯ä¸ªèŠ‚ç‚¹çš„ GPU æ•°é‡
* `${NODE_RANK}` ä¸ºæœ¬èŠ‚ç‚¹çš„ rank

#### torch.distributed.launch
```shell
$ python3 -m torch.distributed.launch --master_addr ${MASTER_ADDR} --master_port ${MASTER_PORT} --nproc_per_node ${GPU_PER_NODE} --nnodes ${NNODES} --node_rank ${NODE_RANK} train.py
```

#### torchrun

```shell
$ torchrun --nnodes=${NNODES} --nproc_per_node=${GPU_PER_NODE} --rdzv_id=1 --rdzv_backend=c10d --rdzv_endpoint=${MASTER_ADDR}:${MASTER_PORT} train.py
```

æ›´å¤šä¿¡æ¯è¯·å‚è€ƒ PyTorch [å®˜æ–¹æ–‡æ¡£](https://pytorch.org/docs/stable/distributed.html#launch-utility)ã€‚

## æ ·ä¾‹

æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªä½¿ç”¨ BMTrain è®­ç»ƒ GPT-2 çš„[æ ·ä¾‹](https://github.com/OpenBMB/BMTrain/tree/main/example)ã€‚
ä»£ç ä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†ã€‚

### ç¬¬ 1 éƒ¨åˆ†: æ¨¡å‹å®šä¹‰

```
â”œâ”€â”€ layers
â”‚   â”œâ”€â”€ attention.py
â”‚   â”œâ”€â”€ embedding.py
â”‚   â”œâ”€â”€ feedforward.py
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ layernorm.py
â”‚   â””â”€â”€ linear.py
â””â”€â”€ models
    â”œâ”€â”€ gpt.py
    â””â”€â”€ __init__.py
```

ä¸Šé¢æ˜¯ä»£ç çš„ç›®å½•ç»“æ„ã€‚

æˆ‘ä»¬å®šä¹‰äº† GPT-2 éœ€è¦çš„æ‰€æœ‰æ¨¡å‹å±‚ï¼Œå¹¶ä½¿ç”¨ BMTrain çš„ `DistributedModule` å’Œ `DistributedParameter` æ¥å¯ç”¨ ZeRO-3 ä¼˜åŒ–ã€‚

### ç¬¬ 2 éƒ¨åˆ†: åˆå§‹åŒ– BMTrain

```python
bmtrain.init_distributed(seed=0)

model = GPT(
    num_layers=8,
    vocab_size=10240, 
    dim_model=2560,
    dim_head=80,
    num_heads=32,
    dim_ff=8192,
    max_distance=1024,
    bias=True,
    dtype=torch.half
)

bmtrain.init_parameters(model) # æˆ–è€…ä½¿ç”¨`bmtrain.load`åŠ è½½checkpoint

# ... å…¶ä»–åˆå§‹åŒ–ï¼ˆä¾‹å¦‚æ•°æ®é›†ï¼‰ ...
```

`bmtrain.init_distributed(seed=0)` ç”¨äºåˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒï¼Œå¹¶è®¾ç½®éšæœºæ•°ç§å­ä¾¿äºå¤ç°ã€‚

`bmtrain.init_parameters(model)` ç”¨äºåˆå§‹åŒ–æ¨¡å‹çš„åˆ†å¸ƒå¼å‚æ•°ã€‚

### ç¬¬ 3 éƒ¨åˆ†: åˆå§‹åŒ–ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒæ•´ç­–ç•¥

```python
loss_func = torch.nn.CrossEntropyLoss(ignore_index=-100)
optimizer = bmtrain.optim.AdamOffloadOptimizer(model.parameters(), weight_decay=1e-2, scale=2**20)
lr_scheduler = bmtrain.lr_scheduler.Noam(optimizer, start_lr=1e-3, warmup_iter=40, end_iter=1000, num_iter=0)
```

BMTrain æ”¯æŒ**æ‰€æœ‰** PyTorch åŸç”Ÿçš„ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°ï¼ŒåŒæ—¶ä½ ä¹Ÿå¯ä»¥ä½¿ç”¨ BMTrain æä¾›çš„èåˆï¼ˆfusedï¼‰ä¼˜åŒ–å™¨ç”¨äºæ··åˆç²¾åº¦è®­ç»ƒã€‚

æ­¤å¤–ï¼Œåœ¨ `bmtrain.lr_scheduler` ä¸­ BMTrain ä¹Ÿæä¾›äº†å¸¸è§çš„å­¦ä¹ ç‡è°ƒæ•´ç­–ç•¥ã€‚

### ç¬¬ 4 éƒ¨åˆ†: è®­ç»ƒ

```python
for iteration in range(1000):
    # ... ä¸ºæ¯ä¸ªrankåŠ è½½æ•°æ® ...

    # æ¢¯åº¦æ¸…é›¶
    optimizer.zero_grad()

    # å‰å‘ä¼ æ’­
    pos = torch.arange(enc_input.size(1)).long().cuda().repeat(enc_input.size(0), 1)
    logits = model(
        enc_input,
        pos,
        pos < enc_length[:, None]
    )
    batch, seq_len, vocab_out_size = logits.size()

    loss = loss_func(logits.view(batch * seq_len, vocab_out_size), targets.view(batch * seq_len))
    
    global_loss = bmtrain.sum_loss(loss).item() # èšåˆæ‰€æœ‰rankä¸Šçš„æŸå¤±

    # æŸå¤±ç¼©æ”¾å’Œåå‘ä¼ æ’­
    loss = optimizer.loss_scale(loss)
    loss.backward()

    # æ›´æ–°å‚æ•°
    bmtrain.optim_step(optimizer, lr_scheduler)

    # ... ä¿å­˜checkpointã€æ‰“å°æ—¥å¿— ...
```

è¿™éƒ¨åˆ†ä»£ç ç•¥æœ‰äº›é•¿ï¼Œä½†å†™èµ·æ¥å°±åƒå¸¸è§çš„è®­ç»ƒä»£ç ä¸€æ ·ï¼Œä½ ä¸éœ€è¦ä¸ºåˆ†å¸ƒå¼è®­ç»ƒè°ƒæ•´å¤ªå¤šçš„ä»£ç ã€‚

ä½ å¯ä»¥æ ¹æ®ä»£ç ä¸­çš„æ³¨é‡Šæ¥äº†è§£å„éƒ¨åˆ†ä»£ç çš„ä½œç”¨ã€‚

å”¯ä¸€éœ€è¦è¯´æ˜çš„æ˜¯ `optimizer.loss_scale`ï¼ŒæŸå¤±ç¼©æ”¾æ˜¯æ··åˆç²¾åº¦è®­ç»ƒä¸­çš„ä¸€é¡¹å¸¸ç”¨æŠ€æœ¯ï¼Œç”¨äºé¿å…æ¢¯åº¦ä¸‹æº¢ã€‚å¦‚æœä½ æ²¡æœ‰ä½¿ç”¨ BMTrain ä¸­çš„èåˆä¼˜åŒ–å™¨ï¼Œä½ å¯ä»¥åˆ é™¤è¿™è¡Œä»£ç ã€‚

<div id="æ€§èƒ½"></div>

## æ€§èƒ½

æˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªæœ‰130äº¿å‚æ•°çš„ GPT-2 æ¨¡å‹ï¼Œä½¿ç”¨äº†4å°æœåŠ¡å™¨ï¼Œæ¯å°æœåŠ¡å™¨æœ‰8å¼ V100æ˜¾å¡ã€‚æˆ‘ä»¬æµ‹è¯•äº†è®­ç»ƒè¿‡ç¨‹ä¸­æ¯ä¸ªGPUçš„ååé‡ï¼ˆæ¯ä¸ªGPUæ¯ç§’å¤„ç†çš„æ ·æœ¬æ•°ï¼‰ï¼Œç»“æœè§ä¸‹è¡¨ã€‚

æ¨¡å‹ç»“æ„ï¼š
* 40å±‚
* 128ä¸ªæ³¨æ„åŠ›å¤´
* 5120çš„éšè—å±‚ç»´æ•°
* 512çš„åºåˆ—é•¿åº¦


| batch size  | 8     | 16    | 24    | 32    |
|-------------|-------|-------|:------|:------|
| BMTrain     | 24.15 | 26.94 | 29.42 | 28.28 |
| ZeRO3(mp=1) | 14.88 | 21.69 | 24.38 | -     |
| ZeRO3(mp=4) | 15.51 | -     | -     | -     |
| ZeRO3(mp=8) | 15.51 | -     | -     | -     |
| ZeRO2(mp=1) | -     | -     | -     | -     |
| ZeRO2(mp=4) | 22.85 | -     | -     | -     |
| ZeRO2(mp=8) | 21.33 | -     | -     | -     |

**ZeROa(mp=b)** è¡¨ç¤º DeepSpeed + Megatron ZeRO stage a å’Œ model parallelism = bã€‚

è¡¨æ ¼ä¸­ **-** è¡¨ç¤ºè¶…å‡ºæ˜¾å­˜ã€‚

## æ¨¡å‹æ”¯æŒ

æˆ‘ä»¬å·²ç»å°†å¤§å¤šæ•°å¸¸è§çš„ NLP æ¨¡å‹ç§»æ¤åˆ°äº† BMTrain ä¸­ã€‚ä½ å¯ä»¥åœ¨ [ModelCenter](https://github.com/OpenBMB/ModelCenter) é¡¹ç›®ä¸­æ‰¾åˆ°æ”¯æŒæ¨¡å‹çš„åˆ—è¡¨ã€‚

## å¼€æºç¤¾åŒº
æ¬¢è¿è´¡çŒ®è€…å‚ç…§æˆ‘ä»¬çš„[è´¡çŒ®æŒ‡å—](https://github.com/OpenBMB/BMTrain/blob/master/CONTRIBUTING.md)è´¡çŒ®ç›¸å…³ä»£ç ã€‚

æ‚¨ä¹Ÿå¯ä»¥åœ¨å…¶ä»–å¹³å°ä¸æˆ‘ä»¬æ²Ÿé€šäº¤æµï¼š
- QQç¾¤: 735930538
- å®˜æ–¹ç½‘ç«™: http://www.openbmb.org
- å¾®åš: http://weibo.cn/OpenBMB
- Twitter: https://twitter.com/OpenBMB

## å¼€æºè®¸å¯

è¯¥å·¥å…·åŒ…ä½¿ç”¨[Apache 2.0](https://github.com/OpenBMB/BMTrain/blob/main/LICENSE)å¼€æºè®¸å¯è¯ã€‚

## å…¶ä»–è¯´æ˜

`BMTrain` å·¥å…·åŒ…å¯¹ PyTorch è¿›è¡Œäº†åº•å±‚ä¿®æ”¹ï¼Œå¦‚æœä½ çš„ç¨‹åºè¾“å‡ºäº†æ„æ–™ä¹‹å¤–çš„ç»“æœï¼Œå¯ä»¥åœ¨issueä¸­æäº¤ç›¸å…³ä¿¡æ¯ã€‚
